# End-to-End-Data-Engineering-Project-using-PySpark
End-to-End Retail Analytics Data Engineering Project using PySpark

***  
I build a complete, production-style retail analytics data engineering pipeline using PySpark on a distributed Spark cluster running in Docker.

We generate 1 million+ retail transactions with intentional data quality issues, process them using Medallion Architecture (Bronze â†’ Silver â†’ Gold), and deliver business-ready Parquet datasets optimized for Power BI dashboards.

This project is designed to demonstrate how Spark is actually used in real-world data engineering â€” not just DataFrame syntax, but distributed execution, schema enforcement, parallel transformations, and analytics modeling.
If you're preparing for Data Engineering, Analytics Engineering, or PySpark-focused roles, this is a serious portfolio project.

## ðŸš€ What Youâ€™ll Learn

In this hands-on project, youâ€™ll learn how to:

â€¢ Generate large-scale raw datasets (1M records) with controlled data impurities  
â€¢ Run a distributed Spark cluster using Docker & Docker Compose  
â€¢ Enforce explicit schemas (no inferSchema shortcuts)  
â€¢ Implement Medallion Architecture properly:  

*Bronze*: raw CSV ingestion with schema enforcement  
*Silver*: data cleaning, deduplication, and business rule validation  
*Gold*: analytics-ready aggregations optimized for BI tools  

â€¢ Handle distributed transformations and wide shuffles  
â€¢ Optimize data storage using Parquet  
â€¢ Control partitions and observe Spark UI execution  
â€¢ Prepare curated datasets for Power BI dashboards  
â€¢ Think like a production data engineer, not a notebook coder  

## ðŸ§  Project Use Case

This project simulates how a retail analytics team processes large-scale transaction data to:

â€¢ Track daily revenue trends  
â€¢ Measure category-level product performance  
â€¢ Analyze city-level revenue distribution  
â€¢ Identify operational and data quality issues  
â€¢ Deliver clean, trusted datasets for business stakeholders  

The goal is not just cleaning data â€” itâ€™s building a scalable, distributed analytics pipeline.

## ðŸ§° Tech Stack Used

* Apache Spark (3.5.x)  
* PySpark  
* Docker & Docker Compose: https://www.docker.com/products/docke...    
* Power BI: https://apps.microsoft.com/detail/9NT...   
* Python  
* VS Code: https://code.visualstudio.com/  

## ðŸ“Š Project Highlights

â€¢ 1,000,000 generated records  
â€¢ Distributed Spark cluster (Master + Workers)  
â€¢ Explicit schema enforcement  
â€¢ Data quality rules and standardization  
â€¢ Parallelized transformations  
â€¢ Business-level aggregations  
â€¢ BI-ready Gold datasets  
â€¢ Clean, modular, portfolio-ready project structure  
